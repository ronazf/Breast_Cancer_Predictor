{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "38d8e323-40b0-4acb-9eb6-0362e67be7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "39a125fb-f2a7-4bce-83b1-1e244e7cff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define activation functions for forward propagation and their respective derivatives for backward propagation\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)), x\n",
    "\n",
    "def sigmoid_backward(gradient, x):\n",
    "    return gradient * (x * (1 - x))\n",
    "\n",
    "def relu(x, alpha = 1):\n",
    "    return np.maximum(alpha * x, x), x\n",
    "\n",
    "def relu_backward(gradient, x, alpha = 1):\n",
    "    x_relu = np.greater(x, 0).astype(int)\n",
    "    x_relu[x == 0] = alpha\n",
    "    return gradient * x_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "a2d7b95a-7ed5-4d6b-a49b-f18751d1eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "# initializes weights and biases of an n-layer neural network\n",
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    layers = len(layer_dims)\n",
    "\n",
    "    for layer in range(1, layers):\n",
    "        parameters['W' + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer - 1]) * 0.01\n",
    "        parameters['b' + str(layer)] = np.zeros((layer_dims[layer], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "9080a082-14d8-44af-b973-e737f32e8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_prop(activations, weights, bias):\n",
    "    pre_act = np.dot(weights, activations) + bias\n",
    "\n",
    "    cache = (activations, weights, bias)\n",
    "\n",
    "    return pre_act, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "020c3ed3-fe69-4c4c-8a1d-dfa4edc6aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_forward_prop(activations, weights, bias, act_fun):\n",
    "    pre_act, linear_cache = linear_forward_prop(activations, weights, bias)\n",
    "    post_act, activation_cache = act_fun(pre_act)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return post_act, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "1facacd5-4099-4906-b90b-8dbfce3aa179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    caches = []\n",
    "    activations = X\n",
    "    layers = len(params) // 2\n",
    "\n",
    "    for layer in range(1, layers):\n",
    "        activations, cache = act_forward_prop(activations, params['W' + str(layer)], params['b' + str(layer)], relu)\n",
    "        caches.append(cache)\n",
    "\n",
    "    output_activations, output_cache = act_forward_prop(activations, params['W' + str(layers)], params['b' + str(layers)], sigmoid)\n",
    "    caches.append(output_cache)\n",
    "\n",
    "    return output_activations, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "5168634f-6aa8-4f9f-b65d-f256b200188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cost(output_activations, Y):\n",
    "    size = Y.shape[1]\n",
    "\n",
    "    cost = (-1 / size) * np.sum((Y * np.log(output_activations)) + ((1 - Y) * np.log(1 - output_activations)))\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "e2ad525d-82ac-4caf-9097-59ac6871ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_back_prop(gradient, cache):\n",
    "    activations, weights, biases = cache\n",
    "    size = activations.shape[1]\n",
    "\n",
    "    weights_deriv = np.dot(gradient, activations.T) / size\n",
    "    biases_deriv = np.sum(gradient, axis = 1, keepdims = True) / size\n",
    "    activations_deriv = np.dot(weights.T, gradient)\n",
    "\n",
    "    return activations_deriv, weights_deriv, biases_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "5d8ad46f-685f-4dff-9601-8c121e4e28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_back_prop(gradient, cache, act_fun):\n",
    "    linear_cache, activation_cache = cache\n",
    "    post_gradient = act_fun(gradient, activation_cache)\n",
    "    cost_gradient, weights_deriv, biases_deriv = linear_back_prop(post_gradient, linear_cache)\n",
    "\n",
    "    return cost_gradient, weights_deriv, biases_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "62de5b47-43bf-4e43-881f-bc1767e89b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(output_activations, Y, caches):\n",
    "    gradients = {}\n",
    "    layers = len(caches)\n",
    "    size = output_activations.shape[1]\n",
    "    Y = Y.reshape(output_activations.shape)\n",
    "\n",
    "    output_act_deriv = - (np.divide(Y, output_activations) - np.divide(1 - Y, 1 - output_activations))\n",
    "\n",
    "    cache = caches[-1]\n",
    "    gradient, weights_deriv, biases_deriv = act_back_prop(output_act_deriv, cache, sigmoid_backward)\n",
    "    gradients['dA' + str(layers - 1)] = gradient\n",
    "    gradients['dW' + str(layers)] = weights_deriv\n",
    "    gradients['db' + str(layers)] = biases_deriv\n",
    "\n",
    "    for layer in reversed(range(layers - 1)):\n",
    "        cache = caches[layer]\n",
    "        gradient, weights_deriv, biases_deriv = act_back_prop(gradient, cache, relu_backward)\n",
    "        gradients['dA' + str(layer)] = gradient\n",
    "        gradients['dW' + str(layer + 1)] = weights_deriv\n",
    "        gradients['db' + str(layer + 1)] = biases_deriv\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "79818e6c-6ea7-4c1b-b292-d6bab853500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate):\n",
    "    parameters = copy.deepcopy(params)\n",
    "    layers = len(parameters) // 2\n",
    "\n",
    "    for layer in range(layers):\n",
    "        parameters['W' + str(layer + 1)] - (learning_rate * grads['dW' + str(layer + 1)])\n",
    "        parameters['b' + str(layer + 1)] - (learning_rate * grads['db' + str(layer + 1)])\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "0fae396e-9c2a-4e00-86b1-6280740d70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, layers_dims, learning_rate = 0.005, iterations = 50000):\n",
    "    np.random.seed(2024)\n",
    "    costs = []\n",
    "\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        output_activations, caches = forward_propagation(X, parameters)\n",
    "        gradients = back_propagation(output_activations, Y, caches)\n",
    "\n",
    "        if i % 100 == 0 or i == iterations:\n",
    "            cost = model_cost(output_activations, Y)\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "bcf27480-2da6-446e-ac7f-fad5509d2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    predictions, caches = forward_propagation(X, parameters)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "9eb895a2-b479-40c6-962f-c048e9583c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    mean = np.mean(data, axis = 0)\n",
    "    std = np.std(data, axis = 0)\n",
    "\n",
    "    return (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "d5fe7acf-139b-4ab8-a973-64ef2500f089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>M</td>\n",
       "      <td>17.20</td>\n",
       "      <td>24.52</td>\n",
       "      <td>114.20</td>\n",
       "      <td>929.4</td>\n",
       "      <td>0.10710</td>\n",
       "      <td>0.18300</td>\n",
       "      <td>0.16920</td>\n",
       "      <td>0.07944</td>\n",
       "      <td>0.1927</td>\n",
       "      <td>...</td>\n",
       "      <td>23.32</td>\n",
       "      <td>33.82</td>\n",
       "      <td>151.60</td>\n",
       "      <td>1681.0</td>\n",
       "      <td>0.1585</td>\n",
       "      <td>0.73940</td>\n",
       "      <td>0.6566</td>\n",
       "      <td>0.18990</td>\n",
       "      <td>0.3313</td>\n",
       "      <td>0.13390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>M</td>\n",
       "      <td>18.31</td>\n",
       "      <td>20.58</td>\n",
       "      <td>120.80</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>0.10680</td>\n",
       "      <td>0.12480</td>\n",
       "      <td>0.15690</td>\n",
       "      <td>0.09451</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>...</td>\n",
       "      <td>21.86</td>\n",
       "      <td>26.20</td>\n",
       "      <td>142.20</td>\n",
       "      <td>1493.0</td>\n",
       "      <td>0.1492</td>\n",
       "      <td>0.25360</td>\n",
       "      <td>0.3759</td>\n",
       "      <td>0.15100</td>\n",
       "      <td>0.3074</td>\n",
       "      <td>0.07863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>B</td>\n",
       "      <td>11.62</td>\n",
       "      <td>18.18</td>\n",
       "      <td>76.38</td>\n",
       "      <td>408.8</td>\n",
       "      <td>0.11750</td>\n",
       "      <td>0.14830</td>\n",
       "      <td>0.10200</td>\n",
       "      <td>0.05564</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>...</td>\n",
       "      <td>13.36</td>\n",
       "      <td>25.40</td>\n",
       "      <td>88.14</td>\n",
       "      <td>528.1</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.28780</td>\n",
       "      <td>0.3186</td>\n",
       "      <td>0.14160</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.09270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>B</td>\n",
       "      <td>12.78</td>\n",
       "      <td>16.49</td>\n",
       "      <td>81.37</td>\n",
       "      <td>502.5</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.05234</td>\n",
       "      <td>0.03653</td>\n",
       "      <td>0.02864</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>13.46</td>\n",
       "      <td>19.76</td>\n",
       "      <td>85.67</td>\n",
       "      <td>554.9</td>\n",
       "      <td>0.1296</td>\n",
       "      <td>0.07061</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.05882</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>0.06410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>B</td>\n",
       "      <td>12.98</td>\n",
       "      <td>19.35</td>\n",
       "      <td>84.52</td>\n",
       "      <td>514.0</td>\n",
       "      <td>0.09579</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.07107</td>\n",
       "      <td>0.02950</td>\n",
       "      <td>0.1761</td>\n",
       "      <td>...</td>\n",
       "      <td>14.42</td>\n",
       "      <td>21.95</td>\n",
       "      <td>99.21</td>\n",
       "      <td>634.3</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>0.32530</td>\n",
       "      <td>0.3439</td>\n",
       "      <td>0.09858</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.09166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "72          M        17.20         24.52          114.20      929.4   \n",
       "516         M        18.31         20.58          120.80     1052.0   \n",
       "469         B        11.62         18.18           76.38      408.8   \n",
       "69          B        12.78         16.49           81.37      502.5   \n",
       "331         B        12.98         19.35           84.52      514.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "72           0.10710           0.18300         0.16920              0.07944   \n",
       "516          0.10680           0.12480         0.15690              0.09451   \n",
       "469          0.11750           0.14830         0.10200              0.05564   \n",
       "69           0.09831           0.05234         0.03653              0.02864   \n",
       "331          0.09579           0.11250         0.07107              0.02950   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "72          0.1927  ...         23.32          33.82           151.60   \n",
       "516         0.1860  ...         21.86          26.20           142.20   \n",
       "469         0.1957  ...         13.36          25.40            88.14   \n",
       "69          0.1590  ...         13.46          19.76            85.67   \n",
       "331         0.1761  ...         14.42          21.95            99.21   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "72       1681.0            0.1585            0.73940           0.6566   \n",
       "516      1493.0            0.1492            0.25360           0.3759   \n",
       "469       528.1            0.1780            0.28780           0.3186   \n",
       "69        554.9            0.1296            0.07061           0.1039   \n",
       "331       634.3            0.1288            0.32530           0.3439   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "72                0.18990          0.3313                  0.13390  \n",
       "516               0.15100          0.3074                  0.07863  \n",
       "469               0.14160          0.2660                  0.09270  \n",
       "69                0.05882          0.2383                  0.06410  \n",
       "331               0.09858          0.3596                  0.09166  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/path/to/data')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "# shuffle dataset\n",
    "df = df.sample(frac=1, random_state=2024)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "3a311416-907a-45dc-aefb-f29802cbbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = standardize(df.loc[:, df.columns != 'diagnosis'].values), np.where(df['diagnosis'] == 'M', 1, 0)\n",
    "# set test sample size\n",
    "test_size = 0.3\n",
    "test_samples = int(len(df) * test_size)\n",
    "\n",
    "train_X = X[test_samples:].reshape(X[test_samples:].shape[0], -1).T\n",
    "train_y = y[test_samples:].reshape((1, y[test_samples:].shape[0]))\n",
    "test_X = X[:test_samples].reshape(X[:test_samples].shape[0], -1).T\n",
    "test_y = y[:test_samples].reshape((1, y[:test_samples].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "63963fd9-82bf-43bf-9be6-368181aafabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### constants ###\n",
    "\n",
    "input_layer_num = X.shape[1]\n",
    "output_layer_num = 1\n",
    "learning_rate = 0.00075\n",
    "\n",
    "layers_dims = [input_layer_num, 20, output_layer_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "ac53344c-776d-496e-84a9-af7bc41e2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = train(train_X, train_y, layers_dims, learning_rate)\n",
    "train_predictions = np.where(predict(train_X, parameters) > 0.5, 1, 0)\n",
    "test_predictions = np.where(predict(test_X, parameters) > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "id": "3c4c0066-9fc7-4f87-b6c7-6116595323ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 94.73684210526315\n",
      "Test accuracy: 92.3529411764706\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = ((np.dot(train_y, train_predictions.T) + np.dot(1 - train_y, 1 - train_predictions.T)) / train_y.size * 100).squeeze()\n",
    "test_accuracy = ((np.dot(test_y, test_predictions.T) + np.dot(1 - test_y, 1 - test_predictions.T)) / test_y.size * 100).squeeze()\n",
    "print('Train accuracy: ' + str(train_accuracy))\n",
    "print('Test accuracy: ' + str(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
